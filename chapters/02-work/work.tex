\chapter {Аналіз алгоритмів}

    В даній роботі розглядається ефективність чотирьох алгоритмів кластеризації. Два із них плоскі: k-means та DBSCAN. Інші два ієрархічні: UPGMA та споріднений із ним алгоритм пошуку найближчого сусіда.

    \section {Опис алгоритмів}
        \paragraph {K-Means}
            K-means --- це метод кластеризації, що повертає таке розбиття, в якому кожен об'єкт належить кластеру із найближчим центром. Задача побудови такого розбиття належить до класу NP \cite{KmeansNpHard}, тому на практиці застосовуються евристичні реалізації.
            Результатом роботи k-means є плоске розбиття. Для роботи алгоритму потрібно заздалегідь володіти інформацією про кількість кластерів, на які розбивається вибірка.
            \begin{algorithm}
                \caption {Алгоритм k-means}
                \begin{enumerate}
                    \item[ ] \emph {Ініціалізація.}
                        Виберемо у довільний спосіб $k$ точок $c_1^{(1)}, c_2^{(1)}, ..., c_k^{(1)}$, що стануть центроїдами кластерів. Номер ітерації $t := 1$.
                    \item
                        Заповнюємо кожен із $k$ кластерів тими об'єктами, для яких його центроїда є найближчою з-поміж центроїд усіх кластерів:
                        \[
                            S_i^{(t)} = \{x_j : ||x_j - c_i^{(t)}|| \leq ||x_j - c_{c^*}^{(t)}||, 
                            i^* = 1, 2, ..., k\}
                        \]
                    \item
                        Порівняємо розподіл об'єктів по кластерах із отриманим на попередньому кроці.
                        Якщо вони збігаються, алгоритм завершено. В іншому разі, переходимо до кроку 3.
                    \item
                        Обчислимо нові центроїди кластерів $c_1^{(t+1)}, c_2^{(t+1)}, ..., c_k^{(t+1)}$ :
                        \[
                            c_i^{(t+1)} = \frac{1} {S_i^{(t)}} \sum_{x_j \in S_i^{(t)}} x_j
                        \]
                        Збільшуємо номер ітерації: $t := t + 1$.
                        Переходимо до кроку 1.                        
                \end{enumerate}
            \end{algorithm}
            
            Оскільки така реалізація є евристичною, результатом її роботи може стати не глобальне, а локальне розбиття. Зокрема, вони залежать від вибору первинних центроїд кластерів на етапі ініціалізації. До цього моменту є декілька відомих підходів, описаних зокрема в \cite{HamerlyKMeansOptimization}. Можливим варіантом є вибір у якості центроїд випадкових об'єктів із вибірки. Інший варіант --- призначення кожного об'єкта у випадковий кластер та вибір центрів цих кластерів як початкових центроїд.
            
            Стандартною практикою при застосуванні k-means є кількаразовий запуск алгоритму із різними вхідними даними та вибір того результату, який повторюється найчастіше.
            
            Для деяких наборів даних k-means збігається за експоненціальний час \cite{KMeansWorstCaseComplexity}, але такі набори на практиці не зустрічаються. Згладжена складність виконання такого алгоритму --- поліноміальна \cite{KMeansSmoothedComplexity}. 
            
            Цей метод має суттєві недоліки. Один із найважливіших --- необхідність задавати кількість кластерів перед запуском. Неправильно обрана кількість кластерів може призвести до некоректного розбиття. Тому після кожного виконання алгоритму потрібно проводити діагностичні перевірки. Крім того, алгоритм розрахований лише на кластери сферичної форми та приблизно однакового розміру. При аналізі вибірки, що містить одне велике скупчення та декілька маленьких, k-means схильний розколювати велике скупчення.
            
        \paragraph {UPGMA}
            Unweighted Pair Group Method with Arithmetic Mean --- алгоритм ієрархічної кластеризації, призначений для побудови філогенетичних дерев. Такі дерева відображають еволюційні взаємозв'язки між різними видами або іншими сутностями, що мають спільного предка.
            
            на початку роботи алгоритму створюється набір кластерів, кожен із яких містить по одному об'єкту вибірки. На кожному кроці здйснюється об'єднання двох найближчих кластерів у один. За відстань між кластерами береться середня відстань між усіма парами їх об'єктів, тобто:
            \[
                d(A, B) = \frac{1}{\mid A \mid \mid B \mid} \sum_{x \in A} \sum_{y \in b} d(x, y)
            \]
            
            Після завершення роботи алгоритму отримується один кластер, який містить усі об'єкти вибірки. Процес об'єднання кластерів під час роботи алгоритму можна зобразити у вигляді дерева. Воно ілюструє зв'язки між кластерами.
            
            UPGMA належить до групи алгоритмів GCP (Global Closest Pair). На кожній ітерації необхідно знайти пару кластерів, найближчих одне до одного. Для цього необхідно мати доступ до матриці відстаней між усіма об'єктами. Очевидно, для досить великих масивів даних обчислення такої матриці займає багато часу, а кешування її потребує великих затрат пам'яті. Використання спеціальної структури даних для зберігання відстаней між об'єктами дозволило реалізувати алгоритм зі складністю $O(n^2)$ \cite{Eppstein} відносно часу, що є очевидною нижньою межею для цього класу алгоритмів.
            
        \paragraph {DBSCAN}
        
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise) --- алгоритм плоскої кластеризації, розроблений у 1996 \cite{DBSCAN}. Це один із найчастіше згадуваних та використовуваних алгоритмів. DBSCAN здійснює розбиття, грунтуючись на густині розташування об'єктів вибірки, а не на відстані до центру. Це дозволяє виділяти кластери довільної форми. Алгоритм розрахований на роботу із даними, що містять певну кількість шуму. Деякі об'єкти вибірки можуть залишились без кластера. Алгоритм не потребує інформації про кількість кластерів.
            
            DBSCAN приймає на вхід два параметри --- $\epsilon$ (окіл, в якому слід шукати сусідів точки) та $n$ (мінімальна кількість сусідів, необхідна для того, щоб точку не визнали шумом). DBSCAN аналізує усі точки вибірки. Ті з них, в $\epsilon$-околі яких є більш ніж $n$ точок, додаються у кластери. Решта визнаються шумом.
            
            \begin{algorithm}
                \caption{Алгоритм DBSCAN}
                \enumerate
                    \item [] \emph{Ініціалізація} Оголосимо лічильник кластерів $i$ рівним нулю.
                    \item Виберемо довільну точку $p$ серед досі не відвіданих. Позначимо $p$ як відвідану.
                    \item Знайдемо всіх її сусідів, що лежать в межах $\epsilon$-околу $p$:
                        \[
                            N = \{x : d(p, x) < \epsilon\}
                        \]
                    \item Якщо $\mid N \mid > n$, створюємо новий кластер, додаємо до нього $p$. Інакше переходимо до кроку 1.
                    \item Вибираємо довільну точку $p'$ серед невідвіданих точок із $N$. Позначаємо її як відвідану.
                    \item Шукаємо всіх її сусідів:
                        \[
                            N' = \{x : d(p', x) < \epsilon\}
                        \]
                    \item Якщо $\mid N' \mid > n$, додаємо $p'$ до кластера та доповнюємо $N$:
                        \[
                            N = N \cup N'
                        \]
                    \item Якщо у N залишились невідвідані точки, переходимо до кроку 4, інакше закриваємо кластер та переходимо до кроку 1.
            \end{algorithm}
            
            DBSCAN на кожній ітерації шукає найближчих сусідів одної з точок вибірки. При прямій реалізації, що передбачає обчислення усього рядка матриці відстаней на кожній ітерації, загальна складність алгоритму становить $O(n^2)$. При використанні індексованої структури даних для збереження матриці відстаней (наприклад, R-дерева, яке дозволяє виконати пошук сусідів в межах деякого околу за $(O(log n))$), загальна складність алгоритму становитиме $O(n log n)$. При використанні неоптимізованої структури даних складність становитиме $O(n^2)$. Щоправда, потреби у пам'яті також виростуть до $O(n^2)$.
        
        \paragraph {Neighbor Joining}
            Це алгоритм ієрархічної кластерихації. Він споріднений із UPGMA, але між ними є суттєві відмінності. Якщо у випадку UPGMA на кожній ітерації ми шукаємо глобально найближчу пару кластерів, то при neighbor joining здіснюється пошук та об'єднання кластерів, які є локально найближчими сусідами одне одного. Для цього достатньо зберігати повний ланцюжок найближчих сусідів. Такий підхід дає можливість вагомо зменшити кількість обчислень. Складність алгоритму визначатиметься кількістю пошуків найближчих сусідів.
            
            \begin{definition}
                Ланцюгом найближчих сусідів називається такий кортеж $P = (C_1, C_2, ..., C_N)$, в якому $C_{i+1}$ є найближчим сусідом $C_i$.
            \end{definition}
            
            \begin{definition}
                Ланцюг найближчих сусідів називається повним, якщо він містить хоча б два кластери, і два останні кластери є взаємно найближчими сусідами.
            \end{definition}
            
            На початку роботи алгоритму вибірка розбивається на кластери за правилом "один об'єкт --- один кластер". Після цього будується повний ланцюг найближчих сусідів. Після цього останні два кластери об'єднуються в один і вилучаються із ланцюга найближчих сусідів, і побудова повного ланцюга найближчих сусідів продовжується. Якщо ланцюг спорожнів, починаємо будувати його знову із довільним чином вибраного кластера.
            
            Додавання одного кластера до ланцюга і перевірка повноти ланцюга має часову складність $O(n)$. Ця операція здійснюється не більш ніж $2(n-1)$ разів. Таким чином, загальна часова складність алгоритму дорівнює $O(n^2)$. 
            
    \section {Тестові дані}
        Ефективність алгоритмів перевірялась на даних Data Mining Cup 2008 року. Це інформація про один із турів німецької державної лотереї. Кожен об'єкт вибірки відображає учасника лотереї. Був проведений збір інформації про учасників лотереї --- зокрема, сімейне та фінансове становище, освіта, рід діяльності та ін. Кількість об'єктів у вибірці --- 113477. Кожен об'єкт вибірки має унікальний числовий ідентифікатор.
        
        Для деяких об'єктів вибірки бракує даних. Для заповнення порожніх клітинок бази було застосовано модифікацію алгоритму ZET, описаного в \cite{Zagorujko}. В оригіналі цей алгоритм здійснює заповнення порожніх клітинок, користуючись інформацією про об'єкти, схожі на той, що містить порожню клітинку. Спочатку вибираються такі рядки, тоді серед цих рядків вибираються стовпці із найвищим коефіцієнтом лінійної регресії відносно атрибута, який потрібно передбачити. В кінці ітерації розв'язується рівняння лінійної регресії, коренем якого є шукане значення атрибута.
        
        Я використав спрощену версію алгоритму, що вибирає значення атрибута як середнє значення у стовпці серед об'єктів, близьких до даного.
        
        Для перевірки алгоритмів також використовувались синтетичні набори даних. Я генерував їх за допомогою утиліти, написаної спеціально із цею методю. Утиліта зчитує із вказаного файла послідовно інформацію про точку, яка буде центром групи об'єктів, та дозволені відхилення від центру по кожній координаті, і генерує вказану кількість об'єктів, розташованих випадковим чином у вказаних межах навколо центра. Задавши достатню кількість центральних точок та підібравши вдалі відхилення, можна добитись набору даних довільної форми. Зокрема, задавши центр угруповання приблизно посередині між усіма іншими точками, і досить великі відхилення по усіх координатах, можна досягнути ефекту шуму в даних. Як і основна програма, утиліта написана на С++.
            
    \section {Реалізація}
        Для перевірки ефективності вищеописаних алгоритмів була створена їх програмна реалізація. У виборі мови програмування я керувався міркуваннями швидкодії та можливості контролю ресурсів комп'ютера. Вибір зупинився на С++. Реалізація являє собою консольну програму без графічного інтерфейсу, що зчитує дані з файла та зберігає результати кластеризації також у файлі.
        
        Створено об'єктно-орієнтовану модель для зберігання об'єктів вибірки. Для зберігання власне даних я використовував стандартні колекції STL. Де-юре STL не належить до стандарту мови С++, але де-факто усі сучасні поширені компілятори мають підтримку цеї бібліотеки. Перша версія програми включала об'єкт ,,DataContainer'', який містив асоціативний контейнер std::map, що дозволяв звертатись до обєктів вибірки по ідентифікатору. Кожен елемент вибірки був об'єктом типу Object, що містив std::vector вказівників на об'єкти класу Attribute. Кожен Attribute містив числове значення відповідного атрибута та інформацію при присутність даного атрибута у відповідного об'єкта вибірки.
        
        Реалізовано також абстрактний клас AbstractMetric, який відповідав метриці простору. За задумом можна було реалізувати довільну метрику простору, а всередині алгоритмів кластеризації абстрагуватись від того, яка конкретно метрика використовується, за допомогою механізму поліморфізму, доступного у С++. В реалізації метрик я зупинився на найпоширенішій на даний момент евклідовій метриці.
        
        Після перших пробних запусків та профілювання я виявив, що найбільше часу витрачається на виконання функції обчислення відстані між об'єктами. Проаналізувавши дані профілювання, я побачив, що левову частку ресурсів займає доступ до атрибутів об'єкта через створену мною структуру даних. Це заставило мене спростити існуючу структуру, видаливши із неї клас Attribute, і залишивши масив чисел, що відповідають атрибутам, як поле класу Object. Інша суттєва проведена мною оптимізація звелась до відмови від використання чисел із плаваючою комою точності (double) та переходу на числа одинарної точності (float). Швидкодія сучасних процесорів при роботі із останніми значно вища, в чому я мав можливість наглядно пересвідчитись.
        
        Наступний етап очевидної оптимізації полягав у відмові від адресації об'єктів по ідентифікатору та переході від асоціативного контейнера до звичайного. Я виявив, що адресація за ідентифікатором не використовується взагалі, проте на неї витрачається багато ресурсів. Перейшовши до адресації за порядковим номером об'єкта в контейнері std::vector, що відтепер містив вказівники на об'єкти, я зекономив досить багато ресурсів.
        
        Подальші оптимізації в більшості стосувались розпаралелення виконання однотипних завдань. Наприклад, задача пошуку сусідів точки в її $\epsilon$-околі дозволяє розбити процес пошуку на декілька взаємонезалежних потоків, кожен з яких обробляв би певну частину вибірки, і по завершенню їх усіх просто об'єднати отримані дані. Оскільки програма виконувалась на двоядерному процесорі, це дозволило наростити продуктивність.
        
        Написання програми відбувалось у текстовому редакторі vim. Використовувався компілятор gcc. Для відлагодження програми я використовував gdb. Відслідковування витрат пам'яті та профілювання програми здійснювалось за допомогою valgrind. При розробці я використовував систему контролю версій git.
