\chapter {Аналіз алгоритмів}

    В даній роботі розглядається ефективність чотирьох алгоритмів кластеризації. Два із них плоскі: k-means та DBSCAN. Інші два ієрархічні: UPGMA та споріднений із ним алгоритм пошуку найближчого сусіда.

\section {Опис алгоритмів}
    \paragraph {K-Means}
        K-means -- це метод кластеризації, що повертає таке розбиття, в якому кожен об'єкт належить кластеру із найближчим центром. Задача побудови такого розбиття належить до класу NP \cite{KmeansNpHard}, тому на практиці застосовуються евристичні реалізації.
        Результатом роботи k-means є плоске розбиття. Для роботи алгоритму потрібно заздалегідь володіти інформацією про кількість кластерів, на які розбивається вибірка.
        \begin{algorithm}
            \caption {Алгоритм k-means}
            \begin{enumerate}
                \item[ ] \emph {Ініціалізація.}
                    Виберемо у довільний спосіб $k$ точок $c_1^{(1)}, c_2^{(1)}, ..., c_k^{(1)}$, що стануть центроїдами кластерів. Номер ітерації $t := 1$.
                \item
                    Заповнюємо кожен із $k$ кластерів тими об'єктами, для яких його центроїда є найближчою з-поміж центроїд усіх кластерів:
                    \[
                        S_i^{(t)} = \{x_j : ||x_j - c_i^{(t)}|| \leq ||x_j - c_{c^*}^{(t)}||, 
                        i^* = 1, 2, ..., k\}
                    \]
                \item
                    Порівняємо розподіл об'єктів по кластерах із отриманим на попередньому кроці.
                    Якщо вони збігаються, алгоритм завершено. В іншому разі, переходимо до кроку 3.
                \item
                    Обчислимо нові центроїди кластерів $c_1^{(t+1)}, c_2^{(t+1)}, ..., c_k^{(t+1)}$ :
                    \[
                        c_i^{(t+1)} = \frac{1} {S_i^{(t)}} \sum_{x_j \in S_i^{(t)}} x_j
                    \]
                    Збільшуємо номер ітерації: $t := t + 1$.
                    Переходимо до кроку 1.                        
            \end{enumerate}
        \end{algorithm}
        
        Оскільки така реалізація є евристичною, результатом її роботи може стати не глобальне, а локальне розбиття. Зокрема, вони залежать від вибору первинних центроїд кластерів на етапі ініціалізації. До цього моменту є декілька відомих підходів, описаних зокрема в \cite{HamerlyKMeansOptimization}. Можливим варіантом є вибір у якості центроїд випадкових об'єктів із вибірки. Інший варіант -- призначення кожного об'єкта у випадковий кластер та вибір центрів цих кластерів як початкових центроїд.
        
        Стандартною практикою при застосуванні k-means є кількаразовий запуск алгоритму із різними вхідними даними та вибір того результату, який повторюється найчастіше.
        
        Для деяких наборів даних k-means збігається за експоненціальний час \cite{KMeansWorstCaseComplexity}, але такі набори на практиці не зустрічаються. Згладжена складність виконання такого алгоритму -- поліноміальна \cite{KMeansSmoothedComplexity}. 
        
        Цей метод має суттєві недоліки. Один із найважливіших -- необхідність задавати кількість кластерів перед запуском. Неправильно обрана кількість кластерів може призвести до некоректного розбиття. Тому після кожного виконання алгоритму потрібно проводити діагностичні перевірки. Крім того, алгоритм розрахований лише на кластери сферичної форми та приблизно однакового розміру. При аналізі вибірки, що містить одне велике скупчення та декілька маленьких, k-means схильний розколювати велике скупчення.
        
    \paragraph {UPGMA}
        Unweighted Pair Group Method with Arithmetic Mean -- алгоритм ієрархічної кластеризації, призначений для побудови філогенетичних дерев. Такі дерева відображають еволюційні взаємозв'язки між різними видами або іншими сутностями, що мають спільного предка.
        
        Алгоритм аналізує структуру, яка описується матрицею відстаней між кластерами, та на її базі створює дендрограму. На кожному кроці алгоритму здйснюється об'єднання двох найближчих кластерів у один. За відстань між кластерами береться середня відстань між всеможливими парами їх об'єктів, тобто:
        \[
            d(A, B) = \frac{1}{\mid A \mid \mid B \mid} \sum_{x \in A} \sum_{y \in b} d(x, y)
        \]
        
        UPGMA належить до групи алгоритмів GCP (Global Closest Pair). На кожній ітерації необхідно знайти пару кластерів, найближчих одне до одного. Для цього на кожній ітерації необхідно мати доступ до матриці відстаней між усіма об'єктами. Очевидно, для досить великих масивів даних обчислення такої матриці займає багато часу, а кешування її потребує великих затрат пам'яті. Використання спеціальної структури даних для зберігання відстаней між об'єктами дозволило реалізувати алгоритм зі складністю $O(n^2)$ \cite{Eppstein}, що є очевидною нижньою межею для цього класу алгоритмів.
        
    \paragraph {DBSCAN}
    
        DBSCAN (Density-Based Spatial Clustering of Applications with Noise) -- алгоритм плоскої кластеризації, розроблений у 1996 \cite{DBSCAN}. Це один із найчастіше згадуваних та використовуваних алгоритмів. DBSCAN здійснює розбиття, грунтуючись на густині розташування об'єктів вибірки, а не на відстані до центру. Це дозволяє виділяти кластери довільної форми. Алгоритм розрахований на роботу із даними, що містять певну кількість шуму. Деякі об'єкти вибірки можуть залишились без кластера. Алгоритм не потребує інформації про кількість кластерів.
        
        DBSCAN приймає на вхід два параметри -- $\epsilon$ (окіл, в якому слід шукати сусідів точки) та $n$ (мінімальна кількість сусідів, необхідна для того, щоб точку не визнали шумом). DBSCAN аналізує усі точки вибірки. Ті з них, в $\epsilon$-околі яких є більш ніж $n$ точок, додаються у кластери. Решта визнаються шумом.
        
        \begin{algorithm}
            \caption{Алгоритм DBSCAN}
            \enumerate
                \item [] \emph{Ініціалізація} Оголосимо лічильник кластерів $i$ рівним нулю.
                \item Виберемо довільну точку $p$ серед досі не відвіданих. Позначимо $p$ як відвідану.
                \item Знайдемо всіх її сусідів, що лежать в межах $\epsilon$-околу $p$:
                    \[
                        N = \{x : d(p, x) < \epsilon\}
                    \]
                \item Якщо $\mid N \mid > n$, створюємо новий кластер, додаємо до нього $p$. Інакше переходимо до кроку 1.
                \item Вибираємо довільну точку $p'$ серед невідвіданих точок із $N$. Позначаємо її як відвідану.
                \item Шукаємо всіх її сусідів:
                    \[
                        N' = \{x : d(p', x) < \epsilon\}
                    \]
                \item Якщо $\mid N' \mid > n$, додаємо $p'$ до кластера та доповнюємо $N$:
                    \[
                        N = N \cup N'
                    \]
                \item Якщо у N залишились невідвідані точки, переходимо до кроку 4, інакше закриваємо кластер та переходимо до кроку 1.
        \end{algorithm}
        
        DBSCAN на кожній ітерації шукає найближчих сусідів одної з точок вибірки. При прямій реалізації, що передбачає обчислення усього рядка матриці відстаней на кожній ітерації, загальна складність алгоритму становить $O(n^2)$. При використанні індексованої структури даних для збереження матриці відстаней (наприклад, R-дерева, яке дозволяє виконати пошук сусідів в межах деякого околу за $(O(log n))$), загальна складність алгоритму становитиме $O(n log n)$. При використанні неоптимізованої структури даних складність становитиме $O(n^2)$. Щоправда, потреби у пам'яті також виростуть до $O(n^2)$.
    
    
