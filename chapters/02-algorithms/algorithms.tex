\chapter {Аналіз алгоритмів}

    В даній роботі розглядається ефективність чотирьох алгоритмів кластеризації. Два із них здійснюють роздільну кластеризацію: k-means та DBSCAN. Інші два --- ієрархічну: UPGMA та споріднений із ним Neighbor-joining.

    \section {Опис алгоритмів}
        \paragraph {K-Means}
            K-means --- це метод кластеризації, що повертає таке розбиття, в якому кожен об'єкт належить кластеру із найближчим центром. Задача побудови такого розбиття належить до класу NP \cite{KmeansNpHard}, тому на практиці застосовуються евристичні реалізації.
            Для роботи алгоритму потрібно заздалегідь володіти інформацією про кількість кластерів, на які розбивається вибірка.
            \begin{algorithm}
                \caption {Алгоритм k-means}
                \begin{enumerate}
                    \item[ ] \emph {Ініціалізація.}
                        Виберемо у довільний спосіб $k$ точок $c_1^{(1)}, c_2^{(1)}, ..., c_k^{(1)}$, що стануть центроїдами кластерів. Номер ітерації $t := 1$.
                    \item
                        Заповнюємо кожен із $k$ кластерів тими об'єктами, для яких його центроїда є найближчою з-поміж центроїд усіх кластерів:
                        \begin{equation}
                            S_i^{(t)} = \{x_j : ||x_j - c_i^{(t)}|| \leq ||x_j - c_{c^*}^{(t)}||, 
                            i^* = 1, 2, ..., k\}
                        \end{equation}
                    \item
                        Порівняємо розподіл об'єктів по кластерах із отриманим на попередньому кроці.
                        Якщо вони збігаються, алгоритм завершено. В іншому разі, переходимо до кроку 3.
                    \item
                        Обчислимо нові центроїди кластерів $c_1^{(t+1)}, c_2^{(t+1)}, ..., c_k^{(t+1)}$ :
                        \begin{equation}
                            c_i^{(t+1)} = \frac{1} {S_i^{(t)}} \sum_{x_j \in S_i^{(t)}} x_j
                        \end{equation}
                        Збільшуємо номер ітерації: $t := t + 1$.
                        Переходимо до кроку 1.                        
                \end{enumerate}
            \end{algorithm}
            
            Оскільки така реалізація є евристичною, результатом її роботи може стати не глобальне, а локальне розбиття. Зокрема, вони залежать від вибору первинних центроїд кластерів на етапі ініціалізації. До цього моменту є декілька відомих підходів, описаних зокрема в \cite{HamerlyKMeansOptimization}. Можливим варіантом є вибір у якості центроїд випадкових об'єктів із вибірки. Інший варіант --- призначення кожного об'єкта у випадковий кластер та вибір центрів цих кластерів як початкових центроїд.
            
            Стандартною практикою при застосуванні k-means є кількаразовий запуск алгоритму із різними вхідними даними та вибір того результату, який повторюється найчастіше.
            
            Для деяких наборів даних k-means збігається за експоненціальний час \cite{KMeansWorstCaseComplexity}, але такі набори на практиці не зустрічаються. Згладжена складність виконання такого алгоритму --- поліноміальна \cite{KMeansSmoothedComplexity}. 
            
            Цей метод має суттєві недоліки. Один із найважливіших --- необхідність задавати кількість кластерів перед запуском. Неправильно обрана кількість кластерів може призвести до некоректного розбиття. Тому після кожного виконання алгоритму потрібно проводити діагностичні перевірки. Крім того, алгоритм розрахований лише на кластери сферичної форми та приблизно однакового розміру. При аналізі вибірки, що містить одне велике скупчення та декілька маленьких, k-means схильний розколювати велике скупчення.
            
        \paragraph {UPGMA}
            Unweighted Pair Group Method with Arithmetic Mean --- алгоритм ієрархічної кластеризації, призначений для побудови філогенетичних дерев. Такі дерева відображають еволюційні взаємозв'язки між різними видами або іншими сутностями, що мають спільного предка.
            
            \begin{algorithm}
                \caption{Алгоритм UPGMA}
                \enumerate
                    \item[ ] \emph{Ініціалізація}
                        Створимо масив кластерів $D$, кожен із яких міститиме по одному об'єкту із вибірки.
                    \item
                        Якщо кількість кластерів рівна одиниці, алгоритм завершено. Інакше, переходимо до кроку 2.
                    \item
                        Знайдемо пару кластерів $(A, B)$ таку, що
                        \begin{equation}
                            d(A, B) = \min_{A, B \in D; A \neq B} d(A, B)
                        \end{equation}
                        де 
                        \begin{equation}
                            d(A, B) = \frac{1}{\mid A \mid \mid B \mid} \sum_{x \in A} \sum_{y \in B} d(x, y)
                        \end{equation}
                    \item
                        Об'єднаємо пару $A, B$ у кластер $C$. Вилучимо $A, B$ із $D$, та додамо $C$ у $D$.
            \end{algorithm}
            
            на початку роботи алгоритму створюється набір кластерів, кожен із яких містить по одному об'єкту вибірки. На кожному кроці здйснюється об'єднання двох найближчих кластерів у один. За відстань між кластерами береться середня відстань між усіма парами їх об'єктів, тобто:
            \begin{equation}
                d(A, B) = \frac{1}{\mid A \mid \mid B \mid} \sum_{x \in A} \sum_{y \in b} d(x, y)
            \end{equation}
            
            Після завершення роботи алгоритму отримується один кластер, який містить усі об'єкти вибірки. Процес об'єднання кластерів під час роботи алгоритму можна зобразити у вигляді дерева. Воно ілюструє зв'язки між кластерами.
            
            UPGMA належить до групи алгоритмів GCP (Global Closest Pair). На кожній ітерації необхідно знайти пару кластерів, найближчих одне до одного. Для цього необхідно мати доступ до матриці відстаней між усіма об'єктами. Очевидно, для досить великих масивів даних обчислення такої матриці займає багато часу, а кешування її потребує великих затрат пам'яті. Використання спеціальної структури даних для зберігання відстаней між об'єктами дозволило реалізувати алгоритм зі складністю $O(n^2)$ \cite{Eppstein} відносно часу, що є очевидною нижньою межею для цього класу алгоритмів.
            
        \paragraph {DBSCAN}
        
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise) \cite{DBSCAN} --- алгоритм роздільної кластеризації, розроблений у 1996. Це один із найчастіше згадуваних та використовуваних алгоритмів. DBSCAN здійснює розбиття, грунтуючись на густині розташування об'єктів вибірки, а не на відстані до центру. Це дозволяє виділяти кластери довільної форми. Алгоритм розрахований на роботу із даними, що містять певну кількість шуму. Деякі об'єкти вибірки можуть залишились без кластера. Алгоритм не потребує інформації про кількість кластерів.
            
            DBSCAN приймає на вхід два параметри --- $\epsilon$ (окіл, в якому слід шукати сусідів точки) та $n$ (мінімальна кількість сусідів, необхідна для того, щоб точку не визнали шумом). DBSCAN аналізує усі точки вибірки. Ті з них, в $\epsilon$-околі яких є більш ніж $n$ точок, додаються у кластери. Решта визнаються шумом.
            
            \begin{algorithm}
                \caption{Алгоритм DBSCAN}
                \enumerate
                    \item Виберемо довільну точку $p$ серед досі не відвіданих. Позначимо $p$ як відвідану.
                    \item Знайдемо всіх її сусідів, що лежать в межах $\epsilon$-околу $p$:
                        \begin{equation}
                            N = \{x : d(p, x) < \epsilon\}
                        \end{equation}
                    \item Якщо $\mid N \mid > n$, створюємо новий кластер, додаємо до нього $p$. Інакше переходимо до кроку 1.
                    \item Вибираємо довільну точку $p'$ серед невідвіданих точок із $N$. Позначаємо її як відвідану.
                    \item Шукаємо всіх її сусідів:
                        \begin{equation}
                            N' = \{x : d(p', x) < \epsilon\}
                        \end{equation}
                    \item Якщо $\mid N' \mid > n$, додаємо $p'$ до кластера та доповнюємо $N$:
                        \begin{equation}
                            N = N \cup N'
                        \end{equation}
                    \item Якщо у N залишились невідвідані точки, переходимо до кроку 4, інакше закриваємо кластер та переходимо до кроку 1.
            \end{algorithm}
            
            DBSCAN на кожній ітерації шукає найближчих сусідів одної з точок вибірки. При прямій реалізації, що передбачає обчислення усього рядка матриці відстаней на кожній ітерації, загальна складність алгоритму становить $O(n^2)$. При використанні індексованої структури даних для збереження матриці відстаней (наприклад, R-дерева, яке дозволяє виконати пошук сусідів в межах деякого околу за $(O(log n))$), загальна складність алгоритму становитиме $O(n log n)$. При використанні неоптимізованої структури даних складність становитиме $O(n^2)$. Щоправда, потреби у пам'яті також виростуть до $O(n^2)$.
        
        \paragraph {Neighbor-joining} \cite{NeighborJoining}
            Це алгоритм ієрархічної кластеризації. Він споріднений із UPGMA, але між ними є суттєві відмінності. Якщо у випадку UPGMA на кожній ітерації ми шукаємо глобально найближчу пару кластерів, то при neighbor joining здіснюється пошук та об'єднання кластерів, які є локально найближчими сусідами одне одного. Для цього достатньо зберігати повний ланцюжок найближчих сусідів. Такий підхід дає можливість вагомо зменшити кількість обчислень. Складність алгоритму визначатиметься кількістю пошуків найближчих сусідів.
            
            \begin{definition}
                Ланцюгом найближчих сусідів називається такий кортеж $P = (C_1, C_2, ..., C_N)$, в якому $C_{i+1}$ є найближчим сусідом $C_i$.
            \end{definition}
            
            \begin{definition}
                Ланцюг найближчих сусідів називається повним, якщо він містить хоча б два кластери, і два останні кластери є взаємно найближчими сусідами.
            \end{definition}
            
            \begin{algorithm}
                \caption {Алгоритм Neighbor-joining}
                \enumerate
                    \item [ ] \emph{Ініціалізація}
                        Створимо множину одинарних кластерів із усіх об’єктів вибірки.
                     \item
                        Виберемо будь-який кластер $p$.
                     \item
                        Побудуємо повний ланцюжок його найближчих сусідів.
                     \item
                        Об’єднаємо два останні кластери у ланцюжку в один та вилучимо їх із ланцюжка.
                     \item
                        Якщо в ланцюжку залишились елементи, продовжимо побудову із останнього елемента та переходимо до кроку 3. Інакше, переходимо до наступного кроку.
                     \item
                        Якщо кількість кластерів більша, ніж 2, переходимо до кроку 1. Інакше, об’єднуємо ці кластери в один. Алгоритм завершено.                      
            \end{algorithm}
            
            На початку роботи алгоритму вибірка розбивається на кластери за правилом "один об'єкт --- один кластер". Після цього будується повний ланцюг найближчих сусідів. Після цього останні два кластери об'єднуються в один і вилучаються із ланцюга найближчих сусідів, і побудова повного ланцюга найближчих сусідів продовжується. Якщо ланцюг спорожнів, починаємо будувати його знову із довільним чином вибраного кластера.
            
            Додавання одного кластера до ланцюга і перевірка повноти ланцюга має часову складність $O(n)$. Ця операція здійснюється не більш ніж $2(n-1)$ разів. Таким чином, загальна часова складність алгоритму дорівнює $O(n^2)$. 
            
    \section{Висновки до розділу 2}
        У розділі 2 описано чотири алгоритми кластеризації: k-means, DBSCAN, UPGMA та neighbor-joining. K-means та DBSCAN належать до класу алгоритмів роздільної кластеризації, а UPGMA та Neighbor-joining здійснюють ієрархічну кластеризацію. 
        
        Кожен із цих алгоритмів має власні недоліки та переваги. Вони стосуються швидкості роботи, вимог до кількості пам’яті, чутливості до форми та розмірів кластерів, здатності розпізнавати та ігнорувати шум в даних. K-means та neighbor-joining --- евристичні алгоритми. Це означає, що результат роботи цих двох алгоритмів не обов’язково буде оптимальним. Вказано оцінку складності алгоритмів та їх потреби у пам’яті. Вказано особливості алгоритмів, їх переваги. Наведено ситуації, в яких найдоцільніше використати конкретний алгоритм.
